Fisher Linear Discriminant Analysis Cheng Li, Bingyu Wang August 31, 2014 1 Whats LDA Fisher Linear Discriminant Analysis (also called Linear Discriminant Analy sis(LDA)) are methods used in statistics, pattern recognition and machine learn ing to nd a linear combination of features which characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classi er, or, more commonly, for dimensionality reduction before later classi cation. LDA is closely related to PCA, for both of them are based on linear, i.e. matrix multiplication, transformations. For the case of PCA, the transformation is baed on minimizing mean square error between original data vectors and data vectors that can be estimated fro the reduced dimensionality data vectors. And the PCA does not take into account any di erence in class. But for the case of LDA, the transformation is based on maximizing a ratio of between-class variance to within-class variance with the goal of reducing data variation in the same class and increasing the separation between classes. Lets see an example of LDA as below(Figure1): Figure 1: LDA examples The left plot shows samples from two classes (depicted in red and blue) along with the histograms resulting from projection onto the line joining the class means. Note that there is considerable class overlap in the projected space. The right plot shows the corresponding projection based on the Fisher linear discriminant, showing the greatly improved class separation. So our job is seeking to obtain a scalar y by projecting the samples X onto a line: y = TX 1 Then try to nd the to maximize the ratio of between-class variance to within-class variance . Next, we will introduce how to use mathematic way to present this problem. 2 Theory and Model To gure out the LDA, rst we need know how to translate between-class variance and within-class variance to mathematic language. Then we try to maximize the ratio between these two. To simplify the problem, we start with two classes problem. 2.1 Two Classes Problem 2.1.1 Head the Problem Assume we have a set of D-dimensional samples X = x(1) x(2) x(m) , N1 of which belong to class C1, and N2 of which belong to class C2. We also assume the mean vector of two classes in X-space: uk = 1 Nk i Ck x(i) where k =12 and in y-space: uk = 1 Nk i Ck y(i) = 1 Nk i Ck Tx(i) = Tuk where k =12 One way to de ne a measure of separation between two classes is to choose the distance between the projected means, which is in y-space, so the between class variance is: u2 u1 = T(u2 u1) Also, we can de ne the within-class variance for each class Ck is: s2 k = (y(i) i Ck uk)2 where k =12 Then, we get the between-class variance and within-class variance, we can de ne our objective function J( ) as: J( ) = (u2 u1)2 s2 1 + s2 2 In fact, if maximizing the objective function J, we are looking for a projection where examples from the class are projected very close to each other and at the same time, the projected means are as farther apart as possible. 2.1.2 Transform the Problem To nd the optimum , we must express J( ) as a function of . Before the optimum,we need introduce scatter instead of variance. We de ne some measures of the scatter as following: 2 The scatter in feature space-x: Sk = i Ck (x(i) uk)(x(i) uk)T Within-class scatter matrix: SW = S1 + S2 Between-class scather matrix: SB = (u2 u1)(u2 u1)T Lets see J( ) again: J( ) = (u2 u1)2 s2 1 + s2 2 The scatter of the projection y can then be expressed as a function of the scatter matrix in feature space x: s2 k = = = (y(i) i Ck i Ck ( Tx(i) uk)2 Tuk)2 T(x(i) i Ck = TSk So we can get: uk)(x(i) s2 1 + s2 2 = TS1 + TS2 = TSW uk)T Similarly, the di erence between the projected means can be expressed in terms of the means in the original feature space: (u2 u1)2 = ( Tu2 Tu1)2 = T(u2 u1)(u2 u1)T = TSB We can nally express the Fisher criterion in terms of SW and SB as: J( ) = TSB TSW Next, we will maximize this objective function. 2.1.3 Solve the Problem The easiest way to maximize the object function J is to derive it and set it to zero. J( ) = ( TSB TSW ) =( TSW ) ( TSB ) ( TSB ) ( TSW ) = =( TSW )2SB ( TSB )2SW =0 3 =0 Divided by TSW : = ( TSW TSW )SB ( = SB JSW =0 TSB TSW )SW =0 = S 1 WSB J =0 = J =S 1 WSB = J =S 1 W(u2 u1)(u2 u1)T = J =S 1 W(u2 u1)((u2 u1)T c R = J =cS 1 W(u2 u1) ) = = c JS 1 W(u2 u1) For now, the problem has been solved and we just want to get the direction of the , which is the optimum : S 1 W(u2 u1) This is known as Fishers linear discriminant(1936), although it is not a dis criminant but rather a speci c choice of direction for the projection of the data down to one dimension, which is y = TX. 2.2 MultiClasses Problem Based on two classes problem, we can see that the shers LDA generalizes grace fully for multiple classes problem. Assume we still have a set of D-dimensional samples X = x(1) x(2) x(m) , and there are totally C classes. Instead of one projection y, mentioned above, we now will seek (C 1) projections [y1 y2 yC 1] by means of (C 1) projection vectors i arranged by columns into a projection matrix = [ 1 2 C 1], where: yi = T iX = y= TX 2.2.1 Derivation First we will use the scatters in space-x as following: Within-class scatter matrix: C SW = i=1 Si where Si = i Ci (x(i) ui)(x(i) ui)T and ui = 1 Ni i Ci x(i) Between-cla